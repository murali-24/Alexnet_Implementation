{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5cd6d",
   "metadata": {},
   "source": [
    "<h2> Image Transformations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ac479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagenet stats\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    # transforms.RandomRotation(degrees=15),\n",
    "])\n",
    "\n",
    "val_transforms =transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46173c",
   "metadata": {},
   "source": [
    "<h2> Dataset Creation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../../datasets/tiny-imagenet-200/train\"\n",
    "VAL_DIR = \"../../datasets/tiny-imagenet-200/val\"\n",
    "\n",
    "train_dataset = ImageFolder(root=TRAIN_DIR, transform=train_transforms)\n",
    "val_dataset = ImageFolder(root=VAL_DIR, transform=val_transforms)\n",
    "# test_dataset = ImageFolder(root=TEST_DIR, transform=val_transforms)\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c71499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706e08f",
   "metadata": {},
   "source": [
    "<h2> Dataset Testing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Image shape: {images.shape}\")\n",
    "    print(f\"Label: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9bfaa",
   "metadata": {},
   "source": [
    "<h2> Alexnet Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925eb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alexnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        feature_extractor_layers = [self.conv1, self.relu, self.maxpool, self.conv2, self.relu,\n",
    "                                    self.maxpool, self.conv3, self.relu, self.conv4, self.relu,\n",
    "                                    self.conv5, self.relu, self.maxpool]\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=9216, out_features=4096)\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n",
    "        self.fc3 = nn.Linear(in_features=4096, out_features=200)\n",
    "\n",
    "        classifier_layers = [self.dropout, self.fc1, self.relu, self.dropout, self.fc2,\n",
    "                             self.relu, self.fc3]\n",
    "        self.classifier = nn.Sequential(*classifier_layers)\n",
    "\n",
    "\n",
    "    def forward(self,images):\n",
    "        out = self.feature_extractor(images)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57492f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b6bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
