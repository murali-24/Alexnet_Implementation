{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5cd6d",
   "metadata": {},
   "source": [
    "<h2> Image Transformations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ac479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagenet stats\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    # transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "])\n",
    "\n",
    "val_transforms =transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46173c",
   "metadata": {},
   "source": [
    "<h2> Dataset Creation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../../datasets/tiny-imagenet-200/train\"\n",
    "VAL_DIR = \"../../datasets/tiny-imagenet-200/val\"\n",
    "\n",
    "train_dataset = ImageFolder(root=TRAIN_DIR, transform=train_transforms)\n",
    "val_dataset = ImageFolder(root=VAL_DIR, transform=val_transforms)\n",
    "# test_dataset = ImageFolder(root=TEST_DIR, transform=val_transforms)\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c71499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706e08f",
   "metadata": {},
   "source": [
    "<h2> Dataset Testing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ed90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Image shape: {images.shape}\") \n",
    "    print(f\"Label: {labels.shape}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9bfaa",
   "metadata": {},
   "source": [
    "<h2> Alexnet Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925eb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alexnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5,stride=1,padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        feature_extractor_layers = [self.conv1, self.relu, self.maxpool, self.conv2, self.relu,\n",
    "                                    self.maxpool, self.conv3, self.relu, self.conv4, self.relu,\n",
    "                                    self.conv5, self.relu, self.maxpool]\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=9216, out_features=4096)\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n",
    "        self.fc3 = nn.Linear(in_features=4096, out_features=200)\n",
    "\n",
    "        classifier_layers = [self.dropout, self.fc1, self.relu, self.dropout, self.fc2,\n",
    "                             self.relu, self.fc3]\n",
    "        self.classifier = nn.Sequential(*classifier_layers)\n",
    "\n",
    "\n",
    "    def forward(self,images):\n",
    "        out = self.feature_extractor(images)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57492f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_of_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "model = Alexnet().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # type: ignore \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3,factor=0.1)\n",
    "checkpoint_path = r\"../checkpoints/current_model.pth\"\n",
    "best_model_path = r\"../checkpoints/best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_one_epoch_summary(epoch,num_of_epochs, train_loss,train_accuracy, val_loss, val_accuracy):\n",
    "    print(f\"\\nEpoch [{epoch}/{num_of_epochs}] Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, history, patience_counter, checkpoint_path):\n",
    "    model.eval()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'history':history,\n",
    "        'patience_counter':patience_counter\n",
    "    }, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(device, model, optimizer, scheduler, checkpoint_path):\n",
    "    print(\"üîÅ Resuming from checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    history = checkpoint['history']\n",
    "    patience_counter = checkpoint['patience_counter']\n",
    "\n",
    "\n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint['epoch']} with val loss {history['val_loss']:.4f}\")\n",
    "    return start_epoch,history,patience_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(device, epoch, num_of_epochs, train_loader, model, criterion, optimizer, clip_value):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (images, labels) in tqdm(enumerate(train_loader),\n",
    "                                                total=len(train_loader), desc=f\"Epoch {epoch} [Train]\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (predictions.argmax(1) == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        if (batch_idx ) % 10 == 0:\n",
    "            tqdm.write(f\"[Train] Epoch [{epoch}/{num_of_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #average loss in an epoch     \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    return avg_train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(device, epoch, num_of_epochs, val_loader, model, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in tqdm(enumerate(val_loader),\n",
    "                                                    total=len(val_loader), desc=f\"Epoch {epoch} [VAL]\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(images)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (predictions.argmax(1) == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "            if (batch_idx ) % 10 == 0:\n",
    "                tqdm.write(f\"[Val] Epoch [{epoch}/{num_of_epochs}], Step [{batch_idx+1}/{len(val_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #average loss in an epoch     \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader,val_loader, criterion, optimizer,scheduler,num_of_epochs,\n",
    "                 device, clip_value=10,checkpoint_path=r\"../checkpoints/current_model.pth\",\n",
    "                  best_model_path=r\"../checkpoints/best_model.pth\", resume=False ):\n",
    "    \n",
    "    start_epoch = 1\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [], \"val_loss\": [],\n",
    "        \"train_acc\": [], \"val_acc\": []\n",
    "    }\n",
    "\n",
    "    early_stop_patience = 8\n",
    "    patience_counter = 0\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        start_epoch,history, patience_counter = load_checkpoint(device, model, optimizer, scheduler, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch, num_of_epochs+1):\n",
    "\n",
    "        avg_train_loss, train_accuracy = train_one_epoch(device, epoch, num_of_epochs, train_loader, model, criterion, optimizer, clip_value)\n",
    "        avg_val_loss, val_accuracy = evaluate_one_epoch(device, epoch, num_of_epochs, val_loader, model, criterion)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"train_acc\"].append(train_accuracy)\n",
    "        history[\"val_acc\"].append(val_accuracy)\n",
    "\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, history, patience_counter, checkpoint_path)\n",
    "\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            model.eval()\n",
    "            save_checkpoint(epoch, model, optimizer, scheduler, history, patience_counter, best_model_path)\n",
    "            print(\"üåü New best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > early_stop_patience:\n",
    "                print(f\"‚èπ Early stopping at epoch {epoch}. No improvement for {early_stop_patience} epochs.\")\n",
    "                break\n",
    "        \n",
    "        log_one_epoch_summary(epoch, num_of_epochs, avg_train_loss, train_accuracy,avg_val_loss, val_accuracy)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90effe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(model, train_loader,val_loader, criterion, optimizer,scheduler,num_of_epochs,\n",
    "                 device, clip_value=10,checkpoint_path=checkpoint_path,best_model_path=best_model_path, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(log_file, hyperparams, metrics):\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            logs = json.load(f)\n",
    "    else:\n",
    "        logs = []\n",
    "\n",
    "    logs.append(log_entry)\n",
    "\n",
    "    with open(log_file, \"w\") as f:\n",
    "        json.dump(logs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_log_file = \"../outputs/experiment_log.json\"\n",
    "csv_log_file = \"../outputs/experiment_log.csv\"\n",
    "\n",
    "hyperparams = {\n",
    "    \"model\": \"AlexNet\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": num_of_epochs,\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"clip_value\": 10,\n",
    "    \"early_stop_patience\": 8,\n",
    "    \"transform\": \"Resize(256)->Crop->Flip->Norm\",\n",
    "    \"Weight Initialization\": None,\n",
    "}\n",
    "\n",
    "final_metrics = {\n",
    "    \"final_train_loss\": history[\"train_loss\"][-1],\n",
    "    \"final_val_loss\": history[\"val_loss\"][-1],\n",
    "    \"final_train_acc\": history[\"train_acc\"][-1],\n",
    "    \"final_val_acc\": history[\"val_acc\"][-1]\n",
    "}\n",
    "\n",
    "log_experiment(json_log_file, hyperparams, final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f90153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_csv(json_path, csv_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        logs = json.load(f)\n",
    "\n",
    "    # Flatten entries (combine hyperparameters and metrics)\n",
    "    flattened_logs = []\n",
    "    for entry in logs:\n",
    "        flat = {\n",
    "            \"timestamp\": entry[\"timestamp\"],\n",
    "            **entry[\"hyperparameters\"],\n",
    "            **entry[\"metrics\"]\n",
    "        }\n",
    "        flattened_logs.append(flat)\n",
    "\n",
    "    df = pd.DataFrame(flattened_logs)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"‚úÖ Log converted to CSV: {csv_path}\")\n",
    "\n",
    "convert_json_to_csv(json_log_file, csv_log_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
